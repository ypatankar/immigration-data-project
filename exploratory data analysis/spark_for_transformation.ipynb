{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, date_add, to_date\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, FloatType as Fl, StringType as St, \\\n",
    "    IntegerType as In, ShortType as SInt, LongType as LInt, DoubleType as Dbl, TimestampType as Tst\n",
    "import pyspark.sql.functions as f \n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from s3fs import S3FileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Access the dynamic setup information from config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# s3fs library used to save files in S3\n",
    "#s3_client = S3FileSystem(anon=False,key=config['AWS']['AWS_ACCESS_KEY_ID'],secret=config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# delete in the end\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                    \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.5\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create spark session\n",
    "    :return: spark session object\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                        \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.5\").enableHiveSupport().getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_sas_labels(filename, outpath):\n",
    "    \"\"\"\n",
    "    Extracts reference data from the SAS Labels Descriptions file.\n",
    "    We will crawl this file and retrieve information about country of citizenship/residence, ports of entry, \n",
    "    state of arrival, mode of transportation, visa types.  \n",
    "    :param filename: name of the file from which data will be extracted\n",
    "    :param outpath: S3 path for cleaned data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    d = dict()\n",
    "\n",
    "    # List of tables to be created from the data extracted\n",
    "    table_list = ['I94CIT', 'I94PORT', 'I94MODE', 'I94ADDR', 'I94VISA']\n",
    "    flag = False\n",
    "    \n",
    "    # read lines from the file and save the information for each table as key, value pair in the dictionary\n",
    "    with open(filename,'r') as fh:\n",
    "        for tab in table_list:\n",
    "            for line in fh:\n",
    "                if tab in line:\n",
    "                    flag = True\n",
    "                    d[tab] = list()\n",
    "                elif '=' in line and 'proc' not in line:\n",
    "                    x = line.split('=')\n",
    "                    d[tab].append([x[0].replace(\"'\",\"\").strip(),x[1].replace(\"'\",\"\").replace(';','').strip()])\n",
    "\n",
    "                if flag and ';' in line:\n",
    "                    flag=False\n",
    "                    break\n",
    "\n",
    "    # iterate over the dictionary to save individual items as separate files on S3\n",
    "    for k, v in d.items():\n",
    "        if k == table_list[0]:\n",
    "            df = pd.DataFrame(v, columns=['cit_res_id','country_name'])\n",
    "            df = df.astype({'cit_res_id':int})\n",
    "            \n",
    "            df.to_csv(\"reference_data/I94CITRES.csv\", index = False)\n",
    "            with s3_client.open(outpath+\"I94CITRES.csv\",'w') as fh:\n",
    "                df.to_csv(fh, index=False)\n",
    "        elif k == table_list[1]:\n",
    "            df = pd.DataFrame(v, columns=['poe_code','city_state'])\n",
    "            df[[\"city\",\"state_or_country\"]] = df.city_state.str.split(', ', expand=True, n=1)\n",
    "            df.drop(columns=['city_state'], inplace=True)\n",
    "            \n",
    "            df.to_csv(\"reference_data/I94PORT.csv\", index = False)\n",
    "            with s3_client.open(outpath+\"I94PORT.csv\",'w') as fh:\n",
    "                df.to_csv(fh, index=False)\n",
    "        elif k == table_list[2]:\n",
    "            df = pd.DataFrame(v, columns=['travel_mode','mode_name'])\n",
    "            \n",
    "            df.to_csv(\"reference_data/I94MODE.csv\", index = False)\n",
    "            with s3_client.open(outpath+\"I94MODE.csv\",'w') as fh:\n",
    "                df.to_csv(fh, index=False)\n",
    "        elif k == table_list[3]:\n",
    "            df = pd.DataFrame(v, columns=['state_code','state_name'])\n",
    "            \n",
    "            # Data cleaning for uniformity\n",
    "            df['state_name'] = df['state_name'].\\\n",
    "            str.replace('N\\.','NORTH').str.replace('S\\.','SOUTH').str.replace('W\\.','WEST').str.replace('DIST\\.','DISTRICT').\\\n",
    "            str.replace(\"WISCONSON\",\"WISCONSIN\")\n",
    "            \n",
    "            df.to_csv(\"reference_data/I94ADDR.csv\", index = False) \n",
    "            with s3_client.open(outpath+\"I94ADDR.csv\",'w') as fh:\n",
    "                df.to_csv(fh, index=False)\n",
    "        elif k == table_list[4]:\n",
    "            df = pd.DataFrame(v, columns=['visa_code','visa_category'])\n",
    "            \n",
    "            df.to_csv(\"reference_data/I94VISA.csv\", index = False)  \n",
    "            with s3_client.open(outpath+\"I94VISA.csv\",'w') as fh:\n",
    "                df.to_csv(fh, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, filename, outpath):\n",
    "    \"\"\"\n",
    "     Processes temperature data.\n",
    "     To address the level of detail, this data set will be filtered to retrieve records 1995 onwards and then aggregating average temperature by month.\n",
    "     The end product of this transformation will be a consolidated table of monthly average temperature information by state, country.   \n",
    "    :param spark: spark session\n",
    "    :param filename: name of the file from which data will be extracted\n",
    "    :param outpath: S3 path for cleaned data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # read temperature data file\n",
    "    df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(filename)\n",
    "    \n",
    "    # extract year and month from dt column\n",
    "    df = df.withColumn(\"year\", year(df.dt)).withColumn(\"month\", month(df.dt))\n",
    "    \n",
    "    # filter data for year greater than 1995 so that the data is more relevant to the current date and only for United States\n",
    "    df = df.select(\"AverageTemperature\",\"State\",\"year\",\"month\").where((f.col(\"year\") >= 1995) & (f.col(\"Country\") == \"United States\"))\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupBy(\"State\",\"month\").avg(\"AverageTemperature\").withColumnRenamed(\"avg(AverageTemperature)\",\"avg_temp\")\n",
    "    \n",
    "    # clean data for extraneous text in state column\n",
    "    df = df.withColumn(\"State\", f.when(f.col(\"State\") == \"Georgia (State)\",\"Georgia\").otherwise(f.col(\"State\")))\n",
    "    \n",
    "    # enrich this data to include state code for the states in USA using the reference data files extracted from SAS labels description\n",
    "    df_state = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"reference_data/I94ADDR.csv\")\n",
    "    df = df.join(df_state, (f.upper(df.State)==f.upper(df_state.state_name)),how='left').drop(\"State\")\n",
    "       \n",
    "    # write temperature data to parquet files and store it on S3\n",
    "    df.write.parquet(outpath, mode=\"overwrite\")\n",
    "    \n",
    "    print(df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[month: int, avg_temp: double, state_code: string, state_name: string]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete this in the end\n",
    "df1 = spark.read.format(\"parquet\").load(\"s3a://dataengineer/temperature/*.snappy.parquet\")\n",
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# delete in the end\n",
    "dummy_window = Window().orderBy(f.lit(1))\n",
    "df = df.withColumn(\"country\",f.split(col(\"iso_region\"),\"-\").getItem(0)).\\\n",
    "withColumn(\"region\",f.split(col(\"iso_region\"),\"-\").getItem(1)).drop(\"iso_country\",\"iso_region\").\\\n",
    "withColumn(\"latitude\",f.split(col(\"coordinates\"),\",\").getItem(0)).\\\n",
    "withColumn(\"longitude\",f.split(col(\"coordinates\"),\",\").getItem(1)).drop(\"coordinates\").\\\n",
    "withColumn(\"airport_id\",f.row_number().over(dummy_window)).\\\n",
    "withColumnRenamed(\"municipality\",\"city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_us_demo_data(spark, filename, outpath):\n",
    "    \"\"\"\n",
    "     Processes US demographic data.\n",
    "     To address the level of detail, this data set will be rolled up to show state-wise demographic information by aggregrating and pivoting the data. \n",
    "     The end product of this transformation will be a consolidated table of demographic information by state.  \n",
    "    :param spark: spark session\n",
    "    :param filename: name of the file from which data will be extracted\n",
    "    :param outpath: S3 path for cleaned data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # read US demographics data and delimit it to separate data in individual columns\n",
    "    df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\";\").option(\"inferSchema\",\"true\").load(filename)\n",
    "    \n",
    "    # rename columns for ease of use\n",
    "    df = df.withColumnRenamed(\"City\",\"city\").withColumnRenamed(\"State\",\"state_name\").withColumnRenamed(\"Median Age\",\"median_age\").\\\n",
    "withColumnRenamed(\"Male Population\",\"male_pop\").withColumnRenamed(\"Female Population\",\"female_pop\").\\\n",
    "withColumnRenamed(\"Total Population\",\"total_pop\").withColumnRenamed(\"Number of Veterans\",\"no_of_vets\").\\\n",
    "withColumnRenamed(\"Foreign-born\",\"foreign_born\").withColumnRenamed(\"Average Household Size\",\"avg_household_size\").\\\n",
    "withColumnRenamed(\"State Code\",\"state_code\").withColumnRenamed(\"Race\",\"race\").withColumnRenamed(\"Count\",\"count\")\n",
    "    \n",
    "    # aggregate and pivot data to adjust the level of granularity of this dataset\n",
    "    df = df.groupBy(\"city\",\"state_name\",\"state_code\",\"median_age\",\"male_pop\",\"female_pop\",\"total_pop\",\"no_of_vets\",\"foreign_born\",\"avg_household_size\").\\\n",
    "pivot(\"race\").agg(f.first(\"count\"))\n",
    "    \n",
    "    # rename the aggregated columns\n",
    "    df = df.withColumnRenamed(\"American Indian and Alaska Native\",\"amer_ind_ak_native\").withColumnRenamed(\"Asian\",\"asian\").\\\n",
    "withColumnRenamed(\"Black or African-American\",\"black\").withColumnRenamed(\"Hispanic or Latino\",\"hisp_latino\").\\\n",
    "withColumnRenamed(\"White\",\"white\")\n",
    "    \n",
    "    # aggregate data by state and apply relevant functions to the numeric columns\n",
    "    df = df.groupBy(\"state_name\",\"state_code\").agg(f.avg(\"median_age\"), f.sum(\"male_pop\"),f.sum(\"female_pop\"),f.sum(\"total_pop\"),f.sum(\"no_of_vets\"),\\\n",
    "f.sum(\"foreign_born\"),f.avg(\"avg_household_size\"),f.sum(\"amer_ind_ak_native\"),f.sum(\"asian\"),f.sum(\"black\"),\\\n",
    "                                     f.sum(\"hisp_latino\"),f.sum(\"white\"))\n",
    "    \n",
    "    # rename the numeric columns\n",
    "    df = df.withColumnRenamed(\"avg(median_age)\",\"median_age\").\\\n",
    "withColumnRenamed(\"sum(male_pop)\",\"male_pop\").withColumnRenamed(\"sum(female_pop)\",\"female_pop\").\\\n",
    "withColumnRenamed(\"sum(total_pop)\",\"total_pop\").withColumnRenamed(\"sum(no_of_vets)\",\"no_of_vets\").\\\n",
    "withColumnRenamed(\"sum(foreign_born)\",\"foreign_born\").withColumnRenamed(\"avg(avg_household_size)\",\"avg_household_size\").\\\n",
    "withColumnRenamed(\"sum(amer_ind_ak_native)\",\"amer_ind_ak_native\").withColumnRenamed(\"sum(asian)\",\"asian\").\\\n",
    "withColumnRenamed(\"sum(black)\",\"black\").withColumnRenamed(\"sum(hisp_latino)\",\"hisp_latino\").\\\n",
    "withColumnRenamed(\"sum(white)\",\"white\")\n",
    "    \n",
    "    # write demographic data to parquet files and store it on S3\n",
    "    df.write.parquet(outpath, mode=\"overwrite\")\n",
    "    \n",
    "    print(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_name: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_pop: long (nullable = true)\n",
      " |-- female_pop: long (nullable = true)\n",
      " |-- total_pop: long (nullable = true)\n",
      " |-- no_of_vets: long (nullable = true)\n",
      " |-- foreign_born: long (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      " |-- amer_ind_ak_native: long (nullable = true)\n",
      " |-- asian: long (nullable = true)\n",
      " |-- black: long (nullable = true)\n",
      " |-- hisp_latino: long (nullable = true)\n",
      " |-- white: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete this in the end\n",
    "df1 = spark.read.format(\"parquet\").load(\"s3a://dataengineer/demographics/part-*.snappy.parquet\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# arrdate and depdate in immigration data are SAS date numeric fields. \n",
    "# Since Jan 1, 1960 the starting point for the SAS date count, we convert arrdate and deptdate to MM-DD-YYYY string format using a udf\n",
    "convert_sas_date = udf(lambda x: (timedelta(days=int(x)) + datetime(1960,1,1)).strftime(\"%m-%d-%Y\") if x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, outpath):\n",
    "    \"\"\"\n",
    "     Processes immigration data.\n",
    "     There are 12 sas7bat files, one for each month of the year 2016. \n",
    "     We notice that the data for June 2016 has more columns than the remaining files and hence need special processing as compared to the other months. \n",
    "     In addition, a few columns will be dropped as they are not relevant to our project.  \n",
    "    :param spark: spark session\n",
    "    :param filename: name of the file from which data will be extracted\n",
    "    :param outpath: S3 path for cleaned data\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # month names for placeholder in filename\n",
    "    months = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "    \n",
    "    # list of columns whose datatype needs to be changed from double to int\n",
    "    double_to_int_columns = ['cicid','i94yr','i94mon','i94cit','i94res','arrdate','i94mode','depdate','i94bir','biryear','i94visa','admnum']\n",
    "    \n",
    "    # dataframe variable\n",
    "    df_spark = ''\n",
    "\n",
    "    # iterate over each month and append data to the dataframe alongwith droping columns and casting columns\n",
    "    for mon in months:\n",
    "        # drop the generic columns across all the sas.dat files\n",
    "        df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_{}16_sub.sas7bdat'.format(mon)).\\\n",
    "        drop('count', 'entdepa', 'entdepd', 'entdepu', 'matflag','insnum','occup')\n",
    "        \n",
    "        # cast the columns\n",
    "        for colu in double_to_int_columns:\n",
    "            df = df.withColumn(colu,f.col(colu).cast(\"int\")) \n",
    "        \n",
    "        # since the file for month jun has extra columns which are not very useful, we drop them\n",
    "        if mon == 'jun':\n",
    "            # since June data has 34 columns we drop them before UNION\n",
    "            df = df.drop('validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa', 'delete_recdup')\n",
    "        \n",
    "        # Duplicating columns to help with bulk loading parquet format to redshift\n",
    "        df = df.withColumn(\"yr\", df[\"i94yr\"]).withColumn(\"mon\", df[\"i94mon\"])\n",
    "        \n",
    "        # if month is jan, then initiate the data frame else append subsequent months' data to the data frame\n",
    "        if mon == 'jan':\n",
    "            df_spark = df\n",
    "        else:  \n",
    "            df_spark = df_spark.union(df)\n",
    "    \n",
    "    # using the udf defined above, convert sas date to gregorian date format and then stored as strings\n",
    "    df_spark = df_spark.withColumn(\"arrdate\",convert_sas_date(df_spark.arrdate)).withColumn(\"depdate\",convert_sas_date(df_spark.depdate))\n",
    "    \n",
    "    # rename the column for ease of understanding\n",
    "    df_spark = df_spark.withColumnRenamed(\"i94bir\",\"age\")\n",
    "    \n",
    "    print(df_spark.count())\n",
    "    \n",
    "    # write immigration data to parquet files and store it on S3 partitioned by year and month on I94 records for faster processing\n",
    "    df_spark.write.parquet(outpath, mode=None, partitionBy=[\"yr\", \"mon\"], compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete this in the end\n",
    "df1 = spark.read.format(\"parquet\").load(\"s3a://dataengineer/immigration/yr=2016/mon=1/part-00000-40106b65-270c-4a38-ba9a-ad3806f3ed9f.c000.snappy.parquet\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40790529\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main method to call supporting data processing functions\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    #extract_sas_labels(config['FILES']['SAS_LABELS'], config['OUTPUT_PATH']['SAS_LABELS_S3PATH'])\n",
    "    #process_us_demo_data(spark, config['FILES']['DEMOGRAPHICS'], config['OUTPUT_PATH']['DEMOGRAPHICS_S3PATH'])\n",
    "    #process_temperature_data(spark, config['FILES']['TEMPERATURE'], config['OUTPUT_PATH']['TEMPERATURE_S3PATH'])\n",
    "    process_immigration_data(spark, config['OUTPUT_PATH']['IMMIGRATION_S3PATH'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('jan', 2847924, 28)\n",
      "('feb', 2570543, 28)\n",
      "('mar', 3157072, 28)\n",
      "('apr', 3096313, 28)\n",
      "('may', 3444249, 28)\n",
      "('jun', 3574989, 34)\n",
      "('jul', 4265031, 28)\n",
      "('aug', 4103570, 28)\n",
      "('sep', 3733786, 28)\n",
      "('oct', 3649136, 28)\n",
      "('nov', 2914926, 28)\n",
      "('dec', 3432990, 28)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "months = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat').\\\n",
    "drop('count', 'entdepa', 'entdepd', 'entdepu', 'matflag','insnum','occup')\n",
    "\n",
    "double_to_int_columns = ['cicid','i94yr','i94mon','i94cit','i94res','arrdate','i94mode','depdate','i94bir','biryear','i94visa','admnum']\n",
    "for colu in double_to_int_columns:\n",
    "    df_spark = df_spark.withColumn(colu,f.col(colu).cast(\"int\")) \n",
    "\n",
    "for mon in months[1:]:\n",
    "    if mon != 'jun':\n",
    "        df = spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "        load('../../data/18-83510-I94-Data-2016/i94_{}16_sub.sas7bdat'.format(mon)).drop('count', 'entdepa', 'entdepd', 'entdepu', 'matflag','insnum','occup')\n",
    "        for colu in double_to_int_columns:\n",
    "            df = df.withColumn(colu,f.col(colu).cast(\"int\")) \n",
    "        df_spark = df_spark.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_jun = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat').\\\n",
    "drop('validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa', 'delete_recdup','count', 'entdepa', 'entdepd', 'entdepu', 'matflag','insnum','occup')\n",
    "\n",
    "for colu in double_to_int_columns:\n",
    "    df_jun = df_jun.withColumn(colu,f.col(colu).cast(\"int\")) \n",
    "\n",
    "df_spark = df_spark.union(df_jun)\n",
    "df_spark = df_spark.withColumnRenamed(\"i94bir\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# arrdate and depdate are SAS date numeric fields. Since Jan 1, 1960 the starting point for the SAS date count, we convert arrdate and deptdate to MM-DD-YYYY string format\n",
    "convert_sas_date = udf(lambda x: (timedelta(days=int(x)) + datetime(1960,1,1)).strftime(\"%m-%d-%Y\") if x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"arrdate\",convert_sas_date(df_spark.arrdate)).withColumn(\"depdate\",convert_sas_date(df_spark.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.write.parquet('s3a://dataengineer/immigration/', mode=None, partitionBy=[\"i94yr\", \"i94mon\"], compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
