{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US immigration I-94 data project\n",
    "\n",
    "#### Project Summary\n",
    "A data analytics platform to draw insights from US immigration data for year 2016, alongwith supporting demographics and temperature data. The goal is to create data pipelines and consolidate data from multiple sources in order to create a single source of truth data store. This data store will facilitate the immigration department make important decisions based on the insights derived from the data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of the project is to deliver a cloud-based data analytics project which will include a high performance data warehouse. The data warehouse can be used to draw insights similar to the ones below:\n",
    "* Does temperature affect the number of visitors to specific states? Are certain states popular during certain seasons?\n",
    "* Is there a correlation between the Median Age in a state and I94 state of arrival?\n",
    "* Number of visitors by visa type/mode of transportation/country of citizenship/country of residence\n",
    "* Which port of entries are busiest?\n",
    "\n",
    "##### Architecture\n",
    "![Architecture](diagrams/Architecture.PNG)\n",
    "\n",
    "Above is the architecture and high-level steps involved:\n",
    "* 1) Data will be read from third party locations\n",
    "* 2) Data will be transformed using Amazon EMR and Apache Spark from raw format to parquet format\n",
    "* 3) Transformed data will be stored on Amazon S3 (intermediate landing zone) \n",
    "* 4) The transformed data in S3 will be loaded in Amazon Reshift data warehouse\n",
    "* 5) Business users can connect directly to Redshift or use a data visualization tool to draw insights from the enriched data.\n",
    "\n",
    "Apache Airflow will be used for workflow management in this project.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### # I94 Immigration Data: \n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from [here](https://travel.trade.gov/research/reports/i94/historical/2016.html). There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"immigration_data_sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### # World Temperature Data: \n",
    "This dataset came from Kaggle which gives average temperature by `state`. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByState.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1855-05-01</td>\n",
       "      <td>25.544</td>\n",
       "      <td>1.171</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855-06-01</td>\n",
       "      <td>24.228</td>\n",
       "      <td>1.103</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1855-07-01</td>\n",
       "      <td>24.371</td>\n",
       "      <td>1.044</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855-08-01</td>\n",
       "      <td>25.427</td>\n",
       "      <td>1.073</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855-09-01</td>\n",
       "      <td>25.675</td>\n",
       "      <td>1.014</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty State Country\n",
       "0  1855-05-01              25.544                          1.171  Acre  Brazil\n",
       "1  1855-06-01              24.228                          1.103  Acre  Brazil\n",
       "2  1855-07-01              24.371                          1.044  Acre  Brazil\n",
       "3  1855-08-01              25.427                          1.073  Acre  Brazil\n",
       "4  1855-09-01              25.675                          1.014  Acre  Brazil"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = pd.read_csv(\"GlobalLandTemperaturesByState.csv\")\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### # U.S. City Demographic Data: \n",
    "This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring;Maryland;33.8;40601;41862;82463;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy;Massachusetts;41.0;44129;49500;93629;41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover;Alabama;38.5;38040;46799;84839;4819;822...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga;California;34.5;88127;87105;1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark;New Jersey;34.6;138040;143873;281913;58...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count\n",
       "0  Silver Spring;Maryland;33.8;40601;41862;82463;...                                                                                                   \n",
       "1  Quincy;Massachusetts;41.0;44129;49500;93629;41...                                                                                                   \n",
       "2  Hoover;Alabama;38.5;38040;46799;84839;4819;822...                                                                                                   \n",
       "3  Rancho Cucamonga;California;34.5;88127;87105;1...                                                                                                   \n",
       "4  Newark;New Jersey;34.6;138040;143873;281913;58...                                                                                                   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_states = pd.read_csv(\"us-cities-demographics.csv\")\n",
    "df_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### # Airport Code Table: \n",
    "This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport = pd.read_csv(\"airport-codes_csv.csv\")\n",
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Please refer `exploratory data analysis/ExploratoryDataAnalysis_*.ipynb` files attached for detailed analysis\n",
    "\n",
    "#### Exploration Summary:\n",
    "* <b>Immigration data</b>: There are 12 sas7bat files, one for each month of the year 2016. Below is the summary of files showing number of rows/columns for each month. We notice that the data for June 2016 has more columns than the remaining files and hence need special processing as compared to the other months. In addition, a few columns will be dropped as they are not relevant to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('month', 'rows', 'columns')\n",
      "('jan', 2847924, 28)\n",
      "('feb', 2570543, 28)\n",
      "('mar', 3157072, 28)\n",
      "('apr', 3096313, 28)\n",
      "('may', 3444249, 28)\n",
      "('jun', 3574989, 34)\n",
      "('jul', 4265031, 28)\n",
      "('aug', 4103570, 28)\n",
      "('sep', 3733786, 28)\n",
      "('oct', 3649136, 28)\n",
      "('nov', 2914926, 28)\n",
      "('dec', 3432990, 28)\n",
      "total row count:  40790529\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                    \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.5\").enableHiveSupport().getOrCreate()\n",
    "months = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "total_row_count = 0\n",
    "print((\"month\",\"rows\",\"columns\"))\n",
    "for mon in months:\n",
    "    df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_{}16_sub.sas7bdat'.format(mon))\n",
    "    total_row_count += df_spark.count()\n",
    "    print((mon, df_spark.count(), len(df_spark.columns)))\n",
    "print(\"total row count: \", total_row_count)\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* <b>US demographics data</b>: This data will be used in relation to the state of arrival column in immigration data. To address the level of detail, this data set will be rolled up to show state-wise demographic information by aggregrating and pivoting. The end product of this transformation will be a consolidated table of demographic information by state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* <b>Global Temperature data by State</b>: This data will be used in relation to the state of arrival column in immigration data. We will aggregate this data because data only until 2013 are provided whereas our immigration data is for 2016. Keeping detailed temperature data will not be very helpful. Hence to address the level of detail, this data set will be filtered to retrieve records 1995 onwards so that it is more relevant to current date and then average temperature by month will be aggregated. The end product of this transformation will be a consolidated table of monthly average temperature information by state, country. We will enrich this data to include state code for the states in USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(645675, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* <b>I94_SAS_Labels_Descriptions </b>: We extract reference information about country of citizenship/residence, ports of entry, state of arrival, mode of transportation, visa types from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* <b>Airport Code data </b>: Since this data does not have direct relation with the immigration data, we will skip this dataset from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "Please see `exploratory data analysis/spark_etl.ipynb` file attached for detailed clean up steps. All the clean data is stored on S3 which will be eventually be loaded to the data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "* <b>SELECT BUSINESS PROCESS: </b>\n",
    "\n",
    "<br /> Analyze US I-94 immigration data for year 2016. We will be focusing on temperature and demographic data in the US for this project.\n",
    "<br />\n",
    "\n",
    "* <b>DECLARE GRAIN:</b>\n",
    "<br /> Immigration data at transaction level .i.e. Entry/exit details of travelers to/from USA\n",
    "<br /> \n",
    "\n",
    "* <b>IDENTIFY DIMENSIONS:</b>\n",
    "<br /> - DIM_DEMOGRAPHICS - Contains demographical information of US states\n",
    "<br /> - DIM_TEMPERATURE - Contains monthly average temperature data for various states in the US\n",
    "<br /> - DIM_I94_CIT_RES - Contains country codes and names for country of citizenship/residence\n",
    "<br /> - DIM_194_ADDR - Contains US state codes and names\n",
    "<br /> - DIM_I94_PORT - Contains ports of entry codes, their cities and state/country\n",
    "<br /> - DIM_I94_MODE - Contains code for mode of travel and description (1-Air, 2-Sea, 3-Land, 9-Not Reported)\n",
    "<br /> - DIM_I94_VISA - Contains visa codes and description (1-Business, 2-Pleasure, 3-Student)\n",
    "<br /> \n",
    "\n",
    "* <b>IDENTIFY FACTS:</b>\n",
    "<br /> - FACT_IMMIGRATION - Contains immigration data at transaction level .i.e. Entry/exit of travelers to the US\n",
    "\n",
    "##### ER Diagram\n",
    "![ER](diagrams/ER.PNG)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Below are the detailed steps\n",
    "* 1) Data will read from third party locations using `spark_etl.py`.\n",
    "* 2) Data will transformed using pandas and Apache Spark from raw format to parquet/csv format through `spark_etl.py`.\n",
    "* 3) Transformed data will be stored on Amazon S3 (intermediate landing zone) using `spark_etl.py`.\n",
    "* 4) The transformed data in S3 will be loaded in Amazon Reshift data warehouse using airflow.\n",
    "\n",
    "`spark_etl.py` will clean and transform data. It will prepare data and store it in intermediate S3 landing zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "612\n",
      "40790529\n"
     ]
    }
   ],
   "source": [
    "%run -i 'spark_etl.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### List of files for running the data pipeline and their purpose:\n",
    "* `spark_etl.py`: Cleans and transforms data to make it ready to be loaded to the data warehouse\n",
    "* `dl.cfg`: Contains all the setup and configuration information for access to cloud solutions\n",
    "* `dags/project_dag.py`: Co-ordinates and runs the data pipeline using airflow\n",
    "* `dags/create_tables.sql`: Contains DDL for creating tables in data warehouse\n",
    "* `plugins/operators/data_quality.py`: Custom airflow operator for running data quality checks after loading data in data warehouse\n",
    "* `plugins/operators/data_quality.py`: Custom airflow operator for for loading data from S3 to Redshift data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Using airflow, the reference tables are created (i.e. SAS labels descriptions data). This is done first because reference data is used for enriching temperature data.\n",
    "Once the reference data is loaded, the next set of dimensions are loaded in Redshift (i.e. temperature data and demographic data).\n",
    "After the dimension tables are loaded, the main fact table is loaded with I94 immigration data.\n",
    "\n",
    "##### Airflow Diagram\n",
    "![airflow](diagrams/airflow.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks are done using an airflow operator. These  include:\n",
    " * Null checks for tables\n",
    " * Source/Target row counts for the tables\n",
    " * Ensuring that all tests passed. If any tests fail, the airflow logs will make a note of which tables quality checks failed.\n",
    " \n",
    "Please see airflow quality check dag operator for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### DIM_I94_CIT_RES: - Contains country codes and names for country of citizenship/residence\n",
    "\n",
    "| Column Name  |   Column Type  |  Description      |\n",
    "| ------------ | -------------- | ------------------|\n",
    "| cit_res_id   |   INT          |  Code for country | \n",
    "| country_name |   VARCHAR      |  Country          |\n",
    "\n",
    "\n",
    "\n",
    "##### DIM_I94_PORT: - Contains ports of entry codes, their cities and state/country\n",
    "\n",
    "|  Column Name     |  Column Type   |  Description                   |\n",
    "| ----| --| ----|\n",
    "|poe_code       |  VARCHAR       | port of entry code             |\n",
    "|city           |  VARCHAR       | City of port of entry          |\n",
    "|state_or_country  |  VARCHAR       | Name of the state oor country  |\n",
    "\n",
    "\n",
    "##### DIM_I94_MODE: - Contains code for mode of travel and description (1-Air, 2-Sea, 3-Land, 9-Not Reported)\n",
    "|  Column Name |  Column Type   |  Description         |  \n",
    "| ------------ | -------------- | ---------------------|  \n",
    "| travel_mode  |   INT          |  ID of the mode      |\n",
    "| mode_name    |   VARCHAR      |  Description of mode | \n",
    "\n",
    "##### DIM_I94_VISA: - Contains visa codes and description (1-Business, 2-Pleasure, 3-Student)\n",
    "|Column Name   |   Column Type  | Description |\n",
    "| ------------ | -------------- | ---------------------|\n",
    "|visa_code     |        INT     |      ID for visa type |\n",
    "|visa_category |       VARCHAR  |     Description of visa type |\n",
    "\n",
    "##### DIM_I94_ADDR: - Contains US state codes and names\n",
    "|Column Name   |   Column Type  | Description          |\n",
    "| ------------ | -------------- | ---------------------|\n",
    "|state_code    |    VARCHAR     |  Code of the state   |\n",
    "|state_name    |    VARCHAR     |  Name of the state   |\n",
    "\n",
    "##### DIM_DEMOGRAPHICS: - Contains demographical information of US states\n",
    "|Column Name            |   Column Type  |  Description   |\n",
    "| ------------ | -------------- | ------------------|                                           \n",
    "|state_name             |    VARCHAR     |   name of the State |\n",
    "|state_code             |    VARCHAR     |   Code of the state |\n",
    "|median_age             |    FLOAT       |   Median Age in the state |\n",
    "|female_pop             |     INT        |   Female population in the state |\n",
    "|total_pop              |     INT        |   Total population in the state |\n",
    "|no_of_vets             |    INT         |   Number of veterans in the state |\n",
    "|foreign_born           |     INT        |   Foreign born veterans in the state |\n",
    "|avg_household_size     |     FLOAT      |    Average family members in the state |\n",
    "|amer_ind_ak_native     |      INT       |   Number of american indian and alaska native in the state |\n",
    "|asian                  |      INT       |    Number of asian in the state |\n",
    "|black                  |      INT       |    Number of black or african american in the state |\n",
    "|white                  |    INT         |    Number of white in the state |\n",
    "\n",
    "##### DIM_TEMPERATURE: - Contains monthly average temperature data for various states in the US at month level\n",
    "|Column Name      |       Column Type  |  Description |\n",
    "| ------------ | -------------- | ------------------|\n",
    "|state_code       |        VARCHAR     |  state code |\n",
    "|state_name       |        VARCHAR     |  State |\n",
    "|month            |            INT     |    Month of the year |\n",
    "|avg_temp         |        FLOAT       | Average Temperature for that month |\n",
    "\n",
    "##### FACT_IMMIGRATION: - Contains immigration data at transaction level .i.e. Entry/exit of travelers to the US\n",
    "|Column Name   |   Column Type  | Description |\n",
    "| ------------ | -------------- | ---------------------|\n",
    "|cicid\t       |    INT         | Primary key ID for record |\n",
    "|i94yr\t       |    INT         | year of entry |\n",
    "|i94mon\t       |    INT         | month of entry |\n",
    "|i94cit\t       |    INT         | Code of country of citizenship |\n",
    "|i94res\t       |    INT         | Code of country of residence |\n",
    "|i94port\t   |        VARCHAR |    Port of entry |\n",
    "|arrdate\t   |        DATE    |    Arrival date in the USA |\n",
    "|i94mode\t   |        INT     |    Mode of travel |\n",
    "|i94addr\t   |        VARCHAR |    US State of arrival |\n",
    "|age \t       |    INT         | Age of Respondent in Years |\n",
    "|i94visa\t   |        INT     |    Visa type |\n",
    "|dtadfile\t   |    VARCHAR     | Character Date Field, Date added to I-94 Files | \n",
    "|biryear\t   |        INT     |    4 digit year of birth |\n",
    "|dtaddto\t   |        VARCHAR |    Character Date Field - Date to which admitted to U.S. (allowed to stay until) |\n",
    "|gender\t       |    VARCHAR     | Gender |\n",
    "|airline\t   |        VARCHAR |    Airline used to arrive in U.S. |\n",
    "|admnum\t        |   INT         | Admission number |\n",
    "|fltno\t       |    VARCHAR     | Flight number of Airline used to arrive in U.S. |\n",
    "|visatype\t   |    VARCHAR     | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* <b>pandas</b>: pandas is a powerful, and easy to use data analysis python library. We used it heavily for our exploratory data analysis portion of the project.\n",
    "* <b>Apache Spark: </b> Spark is an distributed data analytics engine. It is highly performant as it provides im-memory storage for intermediate computations and transformations of data. The computations are built as DAGS which executes the queries in optimized manner. In this project, Spark was very helpful as it provided efficient way for cleaning and transforming large amounts of data, which included I94 immigration data containing 40 million records\n",
    "* <b>Amazon S3: </b> S3 is a highly reliable, available, scalable object storage system in cloud. S3 was used in this project as an intermediate landing zone primarily because it is easily accessible and also it can store large amounts of data in various data formats efficiently.\n",
    "* <b>Amazon EMR: </b> EMR is a big data platform which can be customized as per the workload at hand. It is highly scalable and performant. In addition, EMR uses EMRFS in the background for file storage, which provides the convenience of storing persistent data in Amazon S3. It was an ideal choice in our case as we are using S3 and Spark which work together seamlessly.\n",
    "* <b>Amazon Redshift: </b> Redshift is a simple, highly-performant, cost-effective cloud data warehousing service that provides MPP, columnar storage and columnar compression. We chose this solution because of the huge analytic workload of the immigration data. It will provide efficient loading and retrieval of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Propose how often the data should be updated and why.\n",
    "* Currently monthly data files are provided, so airflow is scheduled to run monthly. Data in intermediate landing zone can be deleted once the quality checks are successful and complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * <b>The data was increased by 100x.</b>\n",
    "   The tools and technologies we have used are cloud-based which are highly scalable, reliable and elastic. These tools can be scaled up or down automatically or on-demand. In addition, if there is a lot of historical data in the data warehouse and cost is a big factor, then data from Redshift can be unloaded to S3 and we can use Redshift Spectrum and leverage external tables if there is a need to query historical data.\n",
    " * <b>The data populates a dashboard that must be updated on a daily basis by 7am every day.</b>\n",
    "   We are using Airflow for workflow management and scheduling of pipelines. Currently, the data pipelines are scheduled to run every month. This schedule can be changed in the airflow setup to run daily.\n",
    " * <b>The database needed to be accessed by 100+ people.</b>\n",
    " If we expect short spikes of usage for ad-hoc queries, then we can leverage Redshift's Short Query Acceleration (SQA) feature.\n",
    " Redshift also provides on-demand scaling. If we expect the number of users to be high for long periods of time, we can resize the cluster based on the demand.\n",
    " In additiona, we can leverage Redshift's workload management(WLM) feature to manage system performance and user experience by configuring concurrent scaling, query queues and user groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
